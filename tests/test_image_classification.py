"""Test for the image classification tasks."""

import json
import os

import pytest

from multimedeval import EvalParams, MultiMedEval, SetupParams

IN_GITHUB_ACTIONS = os.getenv("GITHUB_ACTIONS") == "true"


# @pytest.mark.skipif(IN_GITHUB_ACTIONS, reason="Test doesn't work in Github Actions.")
@pytest.mark.parametrize(
    "batcher_answer, expected_accuracy, expected_macro_f1, expected_macro_auc",
    [
        ("choroidal neovascularization", 0.25, 0.1, 0.5),
        ("diabetic macular edema", 0.25, 0.1, 0.5),
        ("drusen", 0.25, 0.1, 0.5),
        ("normal", 0.25, 0.1, 0.5),
    ],
)
def test_image_classification(
    batcher_answer, expected_accuracy, expected_macro_f1, expected_macro_auc
):
    """Tests the image classification task.

    Args:
        batcher_answer: Answers generated by the batcher.
        expected_accuracy: Expected accuracy.
        expected_macro_f1: Expected macro F1.
        expected_macro_auc: Expected macro AUC.
    """

    def batcher(prompts):
        return [batcher_answer for _ in range(len(prompts))]

    engine = MultiMedEval()

    config_file_path = (
        "tests/test_config.json" if IN_GITHUB_ACTIONS else "MedMD_config.json"
    )
    with open(config_file_path, encoding="utf-8") as config_file:
        config = json.load(config_file)
    engine.setup(SetupParams(mnist_oct_dir=config["mnist_oct_dir"]))

    results = engine.eval(["OCTMNIST"], batcher, EvalParams())

    print(results)

    if "OCTMNIST" not in results:
        # Find the element in the list that has that "name" metrics_OCTMNIST
        raise AssertionError()

    assert (results["OCTMNIST"]["Accuracy"] - expected_accuracy) < 0.01
    assert (results["OCTMNIST"]["F1-macro"] - expected_macro_f1) < 0.01
    assert (results["OCTMNIST"]["AUC-macro"] - expected_macro_auc) < 0.01
