srun: job 7122290 queued and waiting for resources
srun: job 7122290 has been allocated resources
srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option

Filesystems usage for user croyer ( uid 646291381 ):
[0m-------------------------------------------------------------------------------------
Directory                 Used   Limit   Used,%         Files     Limit
[0m-------------------------------------------------------------------------------------
/home/croyer             1.1GB[0m[1m    15GB[0m     6.9%[0m         24869[0m[1m    100000[0m
/data/croyer            [1m[33m 199GB[0m[1m   200GB[0m[1m[33m    99.3%[0m        300194[0m[1m          [0m
/scratch/croyer           55KB[0m[1m    20TB[0m     0.0%[0m            34[0m[1m          [0m
[0m
/shares/menze.dqbm.uzh    73TB[0m[1m        [0m         [0m       9425876[0m[1m          [0m
[0m-------------------------------------------------------------------------------------

!! Please note: the scratch filesystem is intended only for temporary storage.
   Files in /scratch/croyer that have not been accessed in more than 30 days
   may be automatically deleted.

Fri Dec  8 12:46:43 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   26C    P8     9W /  70W |      2MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[nltk_data] Downloading package stopwords to /home/croyer/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package punkt to /home/croyer/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package wordnet to /home/croyer/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!


Running MultiMedBenchmark with Params(usepytorch=True, seed=42, batch_size=64, run_name='run 2023-12-08 12:46:43.991658')
RadGraph already downloaded
Chexbert already downloaded
***** Benchmarking : MIMIC_CXR report generation *****
Generating reports:   0%|          | 0/55 [00:00<?, ?it/s]Generating reports:   0%|          | 0/55 [00:00<?, ?it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Tokenizing report impressions. All reports are cut off at 512 tokens.
  0%|          | 0/64 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 676.93it/s]
Using 1 GPUs!

Begin report impression labeling. The progress bar counts the # of batches completed:
The batch size is 18
  0%|          | 0/4 [00:00<?, ?it/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.83it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.89it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.26it/s]

Tokenizing report impressions. All reports are cut off at 512 tokens.
  0%|          | 0/64 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 688.72it/s]
Using 1 GPUs!

Begin report impression labeling. The progress bar counts the # of batches completed:
The batch size is 18
  0%|          | 0/4 [00:00<?, ?it/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.34it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.18it/s]
/shares/menze.dqbm.uzh/corentin/RadGraph/scorers/RadGraph/allennlp/nn/util.py:1467: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1
{'type': 'json', 'name': 'metrics_MIMIC_CXR report generation', 'value': {'bleu1': 1.0, 'bleu4': 1.0, 'rougeL': {'rougeL_fmeasure': tensor(1.), 'rougeL_precision': tensor(1.), 'rougeL_recall': tensor(1.)}, 'f1-radgraph': 1.0, 'CheXBert vector similarity': 1.0, 'f1-bertscore': 0.9999998807907104, 'radcliq': -0.24213302824961133, 'meteor': 0.999996857114167}}
